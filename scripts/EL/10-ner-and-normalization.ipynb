{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe3fff9c-7756-4d6c-a549-2f5dd131b447",
   "metadata": {},
   "source": [
    "# 10. Prepare data for cell-entity annotation taks\n",
    "Prepare the json of the each page to perform entity linking. \n",
    "\n",
    "Include :\n",
    "* idem and ditto replacement (for pages created for annotations)\n",
    "* named entity recognition on cells who might require it\n",
    "* plot numbers post-treatement [UNDER DEVELOPPEMENT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b23d19-6583-484e-be74-91bd8f9dca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24ceba8c-b214-4a05-823a-e9bc57d2e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "# Access to the utils directory\n",
    "current_dir = os.getcwd()\n",
    "utils_dir = os.path.join(current_dir, '..', 'utils')\n",
    "sys.path.append(utils_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7157ff1-4ea4-4d35-882b-7e72a67a2d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/STual/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/STual/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from string_utils import TableValuesPostTreatment, NormalizeText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "086d639e-4b19-4a3c-ad30-33eac5f3b8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages : 26\n"
     ]
    }
   ],
   "source": [
    "ROOT = \"/home/STual/DAN-cadastre/inference/LHAY/\"\n",
    "JSONS = glob.glob(ROOT + '*.json')\n",
    "print(\"Number of pages : \" + str(len(JSONS)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d98cef44-295b-435e-af47-9f65b6b74d97",
   "metadata": {},
   "source": [
    "## 1. Create entities key"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aefc756d-d0d1-4481-90da-60b0a4862803",
   "metadata": {},
   "source": [
    "from string_utils import create_split_dictionary\n",
    "import uuid\n",
    "\n",
    "chars_to_split = [\"Ⓐ\",\"Ⓑ\",\"Ⓒ\",\"Ⓓ\",\"Ⓔ\",\"Ⓕ\",\"Ⓖ\"]\n",
    "\n",
    "for JSON in JSONS[2:]:\n",
    "    with open(JSON,'r',encoding='utf8') as f:\n",
    "        page = json.load(f)\n",
    "    page_uuid = JSON.replace(ROOT,\"\").replace('.json','')\n",
    "    \n",
    "    page[\"entities\"] = []\n",
    "    for o in page[\"objects\"]:\n",
    "        text = o[\"text\"].replace('ⒼⒼ','Ⓖ')\n",
    "        entities_dict = create_split_dictionary(text, chars_to_split)\n",
    "        #entities_dict[\"uri\"] = uuid.uuid4()\n",
    "        page[\"entities\"].append(entities_dict)\n",
    "\n",
    "    #Deal with items and ditto\n",
    "    page[\"entities\"] = TableValuesPostTreatment.process_idem_ditto_replacements(page[\"entities\"], SPECIAL_VALUE=\"MISSING\", \n",
    "                                    idem_list=['§', 'Ø', 'id', 'idem', 'le meme', 'la meme', 'les memes'],\n",
    "                                    ditto_list=['☼'])\n",
    "    json_path = os.path.join(ROOT, page_uuid + '.json')\n",
    "    \n",
    "    with open(json_path,'w', encoding='utf-8') as f:\n",
    "        json.dump(page, f, ensure_ascii=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cac140-850d-4869-b1a6-762c39c6ab53",
   "metadata": {},
   "source": [
    "## 1. Named Entity Recognition (NER)\n",
    "This step aims to structure more precisly the content of a cell. In this example, its the taxpayers cells how are treated by a fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "997e6187-c046-41c9-9b61-6042068b4d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/STual/DAN-cadastre/outputs/NER_inference/tmp/ner-joint-labelling-io-1534/checkpoint-900')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE = Path(os.path.dirname(os.path.realpath(\"__file__\"))).resolve()\n",
    "OUT_BASE = Path('/home/STual/DAN-cadastre/outputs/NER_inference').resolve()\n",
    "\n",
    "### Model to run\n",
    "MODEL_PATH = OUT_BASE / 'tmp'\n",
    "\n",
    "FINETUNED_MODEL = MODEL_PATH / \"ner-joint-labelling-io-1534/checkpoint-900\"\n",
    "FINETUNED_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d18993-4cf6-4b8d-8426-2d65e8fdefb0",
   "metadata": {},
   "source": [
    "### 1.1 Load NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3112bc6-1219-4022-9b42-805e2d41ad68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/STual/.venv/venv_sti/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
    "\n",
    "LIMIT = 100\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL) #Param : tokenizer du modele souhaité\n",
    "model = AutoModelForTokenClassification.from_pretrained(FINETUNED_MODEL) #Modèle choisi\n",
    "\n",
    "#Classification des entités\n",
    "nlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", use_fast=True, ignore_labels=[\"O+O\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa0f64-4f3f-471f-a185-ddb4c78a95ee",
   "metadata": {},
   "source": [
    "### 1.2 Retrieve distinct mentions in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e25272da-49b4-455c-ae4d-853b8da76710",
   "metadata": {},
   "outputs": [],
   "source": [
    "CELL_ENTITY_TOKEN = \"Ⓒ\" #Token of the column to treat : here it's the taxpayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ea5942f-c055-41dd-a646-20747850bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "column_values = []\n",
    "for JSON in JSONS:\n",
    "    with open(JSON,'r',encoding='utf8') as f:\n",
    "        page = json.load(f)\n",
    "    page_uuid = JSON.replace(ROOT,\"\").replace('.json','')\n",
    "    \n",
    "    #Retrieve all the taxpayers text of the page\n",
    "    column_values_by_page = []\n",
    "    counter = 0\n",
    "    for line in page[\"entities\"]:\n",
    "        keys = line.keys()\n",
    "        if CELL_ENTITY_TOKEN in keys:\n",
    "            column_values_by_page.append([counter, line[CELL_ENTITY_TOKEN]])\n",
    "        else:\n",
    "            column_values_by_page.append([counter, \"MISSING\"])\n",
    "        counter += 1\n",
    "    column_values.append([page_uuid, column_values_by_page])\n",
    "#This return a list [page_uuid,line,list_od_taxpayers]. Idem and Ditto should have been replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110dac2c-7b36-4cd4-a98e-3b42cd058d52",
   "metadata": {},
   "source": [
    "This return a list [page_uuid, line, list_od_taxpayers]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3805d815-e2bb-4e52-a114-f0d826ec2fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 'MISSING']\n"
     ]
    }
   ],
   "source": [
    "#List the distinct taxpayers values \n",
    "start = time.time()\n",
    "distinct_cell_values = []\n",
    "for page in column_values:\n",
    "    for line in page[1]:\n",
    "        try:\n",
    "            if line[1][\"interpreted_text\"] not in distinct_cell_values:\n",
    "                distinct_cell_values.append(line[1][\"interpreted_text\"])\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e038319-f28e-42a2-bda6-06218ef31820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Godefroy denis V↑e↓',\n",
       " 'Gabillot claude',\n",
       " 'mul',\n",
       " 'Nul',\n",
       " 'Benoist (lanoslas',\n",
       " 'Chevalier louis lt↑re↓',\n",
       " 'Massuet denis marie',\n",
       " 'Serre de Saint roman',\n",
       " 'Chanevas louis',\n",
       " 'Sevin jean achille',\n",
       " 'Jacquet Jean ch↑es↓',\n",
       " 'Lebourtier à Fresnes',\n",
       " 'peuvier Et↑e↓',\n",
       " 'Joseph micolast e',\n",
       " 'Roret jacques michel',\n",
       " 'Desaintpaul Bonisanc→V↑e↓',\n",
       " 'Leclerc claude félix',\n",
       " 'Chevreul michel',\n",
       " 'Baschoux denis',\n",
       " 'Demay marie f↑ois↓',\n",
       " 'Delaunay J↑n↓ b↑te↓',\n",
       " 'Privière antoine germain',\n",
       " 'Vincent f↑ois↓ mathury',\n",
       " 'Brice J↑n↓ fils à la→Branve f↑re↓',\n",
       " 'Goyard michel V↑e↓',\n",
       " 'Thibault marie J↑n↓',\n",
       " 'Mateu j↑ques↓ p↑re↓',\n",
       " 'Privière antoine',\n",
       " 'Piviere andré louis',\n",
       " 'Bronzac p↑re↓ marie',\n",
       " 'Maucait antoine',\n",
       " 'Michaux Carrier',\n",
       " 'Deu f↑ois↓ laurent',\n",
       " 'Micolas antoine',\n",
       " 'Deschamps J↑n↓ b↑te↓',\n",
       " 'Trottier J↑ques↓ J↑n↓',\n",
       " 'Boncorps marie antoine',\n",
       " 'hillet a chevilly',\n",
       " 'T horin antoine',\n",
       " 'Godefroy claude joseph',\n",
       " 'David Etie ne philippe',\n",
       " 'Lampy à Versaillat',\n",
       " 'Mamtzay marie geune',\n",
       " 'Cuszegeux F↑ois↓',\n",
       " 'Benoist slanislac',\n",
       " 'Denoist slanislas',\n",
       " 'Tesler agoutilly',\n",
       " 'Lozet henry',\n",
       " 'Grenier louis',\n",
       " 'Godefroy antoine',\n",
       " 'Devernouailles à Vitry',\n",
       " 'Bourdillat J↑n↓',\n",
       " 'Thibautt Jean Elaude (↑pe↓',\n",
       " 'Chevallier Claudet↑e↓',\n",
       " 'Serre de tt Momag s52',\n",
       " 'Chevalier claude fils→S↑tre↓',\n",
       " 'Chevalier claude fils→(demalier claude fils',\n",
       " 'Ddenoist stanislas',\n",
       " 'Chevalier claude fils→Sutve',\n",
       " 'Chevalier claude fils→(denoist stanislas',\n",
       " 'Denoist stanislas',\n",
       " 'Bleuze micolas',\n",
       " 'huard f↑ois↓ j↑h↓',\n",
       " 'Frotier F↑ois↓',\n",
       " 'heuxtaux J↑ques↓ mel',\n",
       " 'Boulogne antoine',\n",
       " 'Chevalier louis',\n",
       " 'Massuet gaspard',\n",
       " 'Micolas jean phitippe',\n",
       " 'Privière claude f↑ois↓',\n",
       " 'Masshet gaspard',\n",
       " 'Micolas Jean Fhilippe',\n",
       " 'Massuet g↑an↓ marie',\n",
       " 'Cornet Veuve',\n",
       " 'Brice louis',\n",
       " 'Micolas jean philippe',\n",
       " 'Frotier V↑e↓',\n",
       " 'Bourrier p↑re↓ honoré',\n",
       " 'Tardu Jerrurier',\n",
       " 'Méry p↑re↓ Jean',\n",
       " 'Micolas antoine V↑e↓',\n",
       " 'Guibert benjamin',\n",
       " 'Deschamps Jn b↑te↓',\n",
       " 'Joseph micolas V↑e↓',\n",
       " 'Vincent f↑ois↓ mathurin',\n",
       " 'Moncouteau p↑re↓ toosar',\n",
       " 'huard f↑ois↓ J↑h↓',\n",
       " 'Privière n↑as↓ michel',\n",
       " 'Plouguet J↑n↓ b↑te↓',\n",
       " 'Bleuze jacques',\n",
       " 'Mezy p↑re↓ Jean',\n",
       " 'Chevallier louis taytor',\n",
       " 'huard Joseph',\n",
       " 'Chapeltier J↑n↓ J↑n↓',\n",
       " 'Diviere theodore',\n",
       " 'Mannel ieu à loy',\n",
       " 'Pleuzy p↑re↓ Jeqne',\n",
       " 'Deurrier andré',\n",
       " 'Leclere claude félix',\n",
       " 'Josephinicolas V↑e↓',\n",
       " 'Bleuze J↑ques↓ noël',\n",
       " 'Rivière thiodore',\n",
       " 'Mainfay p↑re↓ ch↑es↓',\n",
       " 'Vincent f↑ois↓ mathueu',\n",
       " 'Privière théodore',\n",
       " 'Darrué J↑ques↓ victor',\n",
       " 'Chantrelle micolas',\n",
       " 'Desamt paul Bonsort',\n",
       " 'Paschou ( denis',\n",
       " 'Besthelot vencent',\n",
       " 'frepin jeanl l↑e↓',\n",
       " 'Mayeux jacques det f↑ois↓',\n",
       " 'Jacquet Jean charles',\n",
       " 'huard f↑ois↓ J↑n↓',\n",
       " 'Godefroy proppice laude',\n",
       " 'Mailliard louis n↑as↓',\n",
       " 'Claudon jean',\n",
       " 'Containe marie et',\n",
       " 'Mil',\n",
       " 'Serre desaint roman',\n",
       " 'Chevalier touis Et↑ee↓',\n",
       " 'Serouger pierre',\n",
       " 'Protier pierre léonard',\n",
       " 'Apert Jacques',\n",
       " 'Mainfray p↑re↓ ch↑es↓',\n",
       " 'Moncouteau p↑re↓ th↑er↓',\n",
       " 'Detouzé (mathuelouis→loussain',\n",
       " 'Gallais J↑n↓ achille',\n",
       " 'Priviere théo dore',\n",
       " 'Riviève andré louis→et Emilebvictor',\n",
       " 'Baschoux Fortine',\n",
       " 'Lecterc pierre louis',\n",
       " 'Deschamps Jn ↑↑te↓',\n",
       " 'Sevin Jean F↑ois↓',\n",
       " 'Chevalier f↑ois↓ claude',\n",
       " 'Aubouin claude th↑es↓',\n",
       " 'Serre dest Roman',\n",
       " 'Chevalier louis Et↑e↓',\n",
       " 'Godefroy denis ch↑es↓ V↑e↓',\n",
       " 'Auboun claude th↑as↓',\n",
       " 'Dergent pierre',\n",
       " 'Dillet à chevilly',\n",
       " 'Parice paulhitor',\n",
       " 'Leduc Edme',\n",
       " 'Prenier louis jésome',\n",
       " 'Massuet louis h↑as↓',\n",
       " 'Pregnault pierre',\n",
       " 'hardmarc V↑e↓ hôtie',\n",
       " 'Brice J↑n↓',\n",
       " 'Chapel lier Jeanpt',\n",
       " 'Chandenier p↑re↓',\n",
       " 'Leduc J↑n↓ F↑ois↓',\n",
       " 'Beurrier pierre',\n",
       " 'Ledic Jean F↑ois↓ V↑e↓',\n",
       " 'Angot J↑n↓ b↑te↓ père',\n",
       " 'Desaunay J↑n↓ b↑te↓',\n",
       " 'Massuet louis michel',\n",
       " 'Beurrier pierre honoré',\n",
       " 'Trotier joseph',\n",
       " 'picard georges',\n",
       " 'Barriné J↑ques↓ victor',\n",
       " 'Godefroy denis ch↑es↓',\n",
       " 'Deu Jean V↑te↓',\n",
       " 'Micolas claude',\n",
       " 'Deu Jean b↑te↓',\n",
       " 'Pivière antoine',\n",
       " 'Manfray ch↑es↓ pierre',\n",
       " 'huard paul f↑ois↓',\n",
       " 'Poot claude f↑ois↓',\n",
       " 'Croux jean jabriel',\n",
       " 'huerd F↑ois↓ j↑h↓',\n",
       " 'aubouin claude thas',\n",
       " 'Luisette louis Cyer',\n",
       " 'Anard J↑n↓',\n",
       " 'Micard georges',\n",
       " 'Miot claude f↑ois↓',\n",
       " 'Moinery antoine',\n",
       " 'Deettrelot simon, V↑e↓',\n",
       " 'pepin Jean b↑te↓',\n",
       " 'Benoist Venistay',\n",
       " 'Mailliard louis micolas',\n",
       " 'Mateux pierre f↑ois↓',\n",
       " 'Marengue Jean Cir',\n",
       " 'Maheux pierre F↑ois↓',\n",
       " 'Defontaine marie anne',\n",
       " 'Chevalier f↑ois↓ Claude',\n",
       " 'Croux Jean gabriel',\n",
       " 'huard J↑ous↓ J↑n↓',\n",
       " 'heurtaux J↑ques↓ michel',\n",
       " 'Coutin jean',\n",
       " 'Lebourher à fresnes',\n",
       " 'Chevalier angélique',\n",
       " 'Lebourlier à fréseric',\n",
       " 'Chevalier loussaint',\n",
       " 'Chevalier louis C↑re↓',\n",
       " 'Favze J↑n↓ F↑ois↓',\n",
       " 'Moncouteau german',\n",
       " 'Moncouteau Jean',\n",
       " 'Desaint paul bonssaint V↑e↓',\n",
       " 'Chevzul michel',\n",
       " 'Sevin Jean achille',\n",
       " 'hepin louis f↑ois↓',\n",
       " 'Degremne hypotiste',\n",
       " 'Degremne gabriel',\n",
       " 'Mainfray pre f↑ois↓',\n",
       " 'Frotier p↑re↓ léonard',\n",
       " 'Barrié J↑ques↓ victor',\n",
       " 'Rague f↑ois↓ hattauit',\n",
       " 'Tehibant louis léonard',\n",
       " 'Gallais marie Eléxabelth',\n",
       " 'Gallais Jean achille',\n",
       " 'Gallais marie llixabette',\n",
       " 'Sursu→Temmas antoine V↑e↓',\n",
       " 'Legendre truve',\n",
       " 'Thibault louis f↑ois↓',\n",
       " 'Barmé J↑ques↓ victor',\n",
       " 'Mayeux louis fils à f↑ois↓',\n",
       " 'hiard marie V↑e↓ trosir',\n",
       " 'Maucuit antoine→Jean',\n",
       " 'hnard varie j↑t↓',\n",
       " 'Cochetin louis marie',\n",
       " 'Trottier J↑h↓ J↑ques↓',\n",
       " 'Chavier pettetor C↑is↓ Oit Ve',\n",
       " 'Carré louis',\n",
       " 'Thibault germain',\n",
       " 'Méry Jran V↑e↓',\n",
       " 'hard f↑ois↓ Joseph',\n",
       " 'Chaneves louis',\n",
       " 'Liou f↑ois↓ V↑e↓',\n",
       " 'Lamy vincent',\n",
       " 'Guibert Benjamin',\n",
       " 'Brice Joseph',\n",
       " 'Rivière antoine N↑as↓',\n",
       " 'Barchoux denis',\n",
       " 'Massuet germard',\n",
       " 'Mateu Jacques F↑ois↓ V↑e↓',\n",
       " 'hnard f↑ois↓ J↑n↓',\n",
       " 'Bronzacpierre',\n",
       " 'Monsson J↑n↓ b↑te↓',\n",
       " 'Denoist Otanislas',\n",
       " 'MISSING',\n",
       " 'Chevalier houssaint',\n",
       " 'Legendre V↑e↓',\n",
       " 'Sureau Emmanuel',\n",
       " 'Pincent f↑ois↓ mathurin',\n",
       " 'Bigot alex↑dre↓ f↑ois↓',\n",
       " 'Cochelin louismarc',\n",
       " 'Jerouge louis→alexandre',\n",
       " 'Godefroy claude→antoine per',\n",
       " 'Lebourtier à Fresne',\n",
       " 'Sevin jean F↑ois↓',\n",
       " 'Gallais marie (léxabetts',\n",
       " 'Desaint Paul Bomtaice→N↑e↓',\n",
       " 'pepin jean b↑te↓',\n",
       " 'Moncouteau germain',\n",
       " 'Claudon jean Marie',\n",
       " 'Rivière andré louis→Et Enite alexlor',\n",
       " 'Sevin louis gabriel',\n",
       " 'Piviere andré louis→ct Emile victor',\n",
       " 'Leclerc claude fils',\n",
       " 'Guitet philippe h↑ers↓',\n",
       " 'Pre gnault fils→',\n",
       " 'Lebourtier n↑as↓',\n",
       " 'Gallais marie Elémbelt',\n",
       " 'Vincent f↑ois↓ mathuerin',\n",
       " 'Chandenies pierre',\n",
       " 'Leclerc claude felix',\n",
       " 'Thibault Jean G↑re↓→claude',\n",
       " 'Sérouge louisalexandre',\n",
       " 'Robert J↑n↓ b↑te↓',\n",
       " 'Lamotte germain',\n",
       " 'Cousté j↑h↓',\n",
       " 'Mayeux louis',\n",
       " 'Reignautt fils',\n",
       " 'Liou J↑n↓ michel',\n",
       " 'Cheva Pier louis Et↑e↓',\n",
       " 'Barrué J↑ques↓ victor',\n",
       " 'Trotier J↑h↓',\n",
       " 'Serouge louis alex↑dre↓',\n",
       " 'Privière louis ch↑es↓',\n",
       " 'Chantrelle Nicolas',\n",
       " 'Cochelin louis marie',\n",
       " 'Massuet denis→marie',\n",
       " 'Main rayp↑re↓ ch↑es↓',\n",
       " 'Coyard michel V↑e↓',\n",
       " 'Rague f↑ois↓ toussaint',\n",
       " 'Sérouge louis alex↑rre↓',\n",
       " 'Leclercclaude félix',\n",
       " 'Massuet dénis marie']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_cell_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e156e-16fa-41d3-8af8-3f11d1bb96a0",
   "metadata": {},
   "source": [
    "### 1.3 Detect crossed-out text and perform NER\n",
    "* Detect crossed out text and separate content of the cell in two parts : crossed-out and non-crossed out text\n",
    "* Proceed to named entities recogniion in both crossed-out and non crossed out text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c9ae287-c79e-4332-959e-e38d1c112692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time for 289 entries is 2.394 seconds (average per entry : 8.284 milliseconds)\n"
     ]
    }
   ],
   "source": [
    "ner_annotated_cells = {}\n",
    "ner_annotated_cells_crossed_out = {}\n",
    "\n",
    "start = time.time()\n",
    "for t in distinct_cell_values :\n",
    "    text, crossed_out = NormalizeText.separate_crossed_out(t)\n",
    "    \n",
    "    # Not crossed out\n",
    "    res = nlp(text)\n",
    "    for elem in res:\n",
    "        elem[\"entity_group\"] = elem[\"entity_group\"].replace('+O','')\n",
    "        elem[\"score\"] = round(float(elem[\"score\"]),4)\n",
    "    ner_annotated_cells.update({t:res})\n",
    "    \n",
    "    #Crossed out\n",
    "    if len(crossed_out) > 0:\n",
    "        res2 = nlp(crossed_out)\n",
    "        for elem in res2:\n",
    "            elem[\"entity_group\"] = elem[\"entity_group\"].replace('+O','')\n",
    "            elem[\"score\"] = round(float(elem[\"score\"]),4)\n",
    "        ner_annotated_cells_crossed_out.update({t:res2})\n",
    "        \n",
    "end = time.time()\n",
    "run = end-start\n",
    "average_runtime = run/len(ner_annotated_cells)\n",
    "print(f\"Running time for {len(ner_annotated_cells)} entries is {round(run,3)} seconds (average per entry : {round(average_runtime*1000,3)} milliseconds)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a79ab-f6d3-4166-b195-413c93686c83",
   "metadata": {},
   "source": [
    "### 1.4 Save the results into the JSONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceaf6cfa-7082-4653-8456-a928e042decf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "column_values = []\n",
    "default_ner_properties = [\"name\",\"firstnames\",\"activity\",\"address\",\"title\",\"birthname\",\"familystatus\"]\n",
    "\n",
    "for JSON in JSONS:\n",
    "    with open(JSON) as f:\n",
    "        page = json.load(f)\n",
    "    page_uuid = JSON.replace(ROOT,\"\").replace('.json','')\n",
    "    \n",
    "    for line in page[\"entities\"]:\n",
    "        if CELL_ENTITY_TOKEN in line.keys():\n",
    "            line[CELL_ENTITY_TOKEN][\"ner_interpreted_text\"] = ner_annotated_cells[line[CELL_ENTITY_TOKEN][\"interpreted_text\"]]\n",
    "            # Group by entity_group and concatenate words\n",
    "            data = line[CELL_ENTITY_TOKEN][\"ner_interpreted_text\"]\n",
    "            grouped = defaultdict(str)\n",
    "            grouped[\"name\"]=\"\"\n",
    "            grouped[\"firstnames\"]=\"\"\n",
    "            grouped[\"birthname\"]=\"\"\n",
    "            grouped[\"address\"]=[]\n",
    "            grouped[\"familystatus\"]=[]\n",
    "            grouped[\"activity\"]=[]\n",
    "            grouped[\"title\"]=[]\n",
    "            for item in data:\n",
    "                if item['entity_group'] in (\"name\",\"firstnames\",\"birthname\"):\n",
    "                    grouped[item['entity_group']] += item['word'] + ' '\n",
    "                else:\n",
    "                    grouped[item['entity_group']].append(item['word'])\n",
    "            \n",
    "            retrieved_entities = grouped.keys()\n",
    "            for ne in default_ner_properties:\n",
    "                if ne not in retrieved_entities:\n",
    "                   grouped[ne] = \"\"\n",
    "    \n",
    "            line[CELL_ENTITY_TOKEN][\"ner_interpreted_text_grouped\"] = grouped\n",
    "            \n",
    "            if line[CELL_ENTITY_TOKEN][\"interpreted_text\"] in ner_annotated_cells_crossed_out.keys():\n",
    "                if len(ner_annotated_cells_crossed_out[line[CELL_ENTITY_TOKEN][\"interpreted_text\"]]) > 0:\n",
    "                    line[CELL_ENTITY_TOKEN][\"ner_interpreted_text_crossed_out\"] = ner_annotated_cells_crossed_out[line[CELL_ENTITY_TOKEN][\"interpreted_text\"]]\n",
    "    \n",
    "    with open(JSON,'w', encoding='utf-8') as f:\n",
    "        json.dump(page, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762072f5-af9f-4ac9-a6a3-e837d3bc41e3",
   "metadata": {},
   "source": [
    "## 2. Item and Dittos propagations among named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d85558-05f1-43e4-8469-8a49e5c21dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bc1df23-64bd-49ce-9980-f2a069365f62",
   "metadata": {},
   "source": [
    "## 3. Numbers post-correction\n",
    "\n",
    "!!!! NOT WORKING !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b28e9cbe-39e1-408d-bdd9-3130276ace73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_and_propose_corrections(plots):\n",
    "    \"\"\"\n",
    "    Detect anomalies in the sequence of plot numbers and propose corrections.\n",
    "    The plots list contains numbers or text errors.\n",
    "    \n",
    "    :param plots: List of plot numbers (or text) from the table.\n",
    "    :return: List of anomalies and proposed corrections.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Helper function to check if a number is an integer\n",
    "    def is_integer(value):\n",
    "        try:\n",
    "            int(value)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    \n",
    "    # Convert all values to integers or NaN if they are invalid\n",
    "    plot_numbers = [int(plot) if is_integer(plot) else None for plot in plots]\n",
    "\n",
    "    # Detect anomalies\n",
    "    anomalies = []\n",
    "    corrected_plots = plot_numbers.copy()\n",
    "\n",
    "    for i in range(1, len(plot_numbers)):\n",
    "        current_plot = plot_numbers[i]\n",
    "        previous_plot = plot_numbers[i-1]\n",
    "        biggest = previous_plot\n",
    "        \n",
    "        # Case 1: Plot is NaN or not an integer\n",
    "        if current_plot is None:\n",
    "            anomalies.append(f\"Anomaly: Invalid value at position {i} (Non-integer or empty).\")\n",
    "            continue\n",
    "        \n",
    "        # Case 2: Plot is out of order (non-increasing or skipped numbers)\n",
    "        if current_plot < previous_plot:\n",
    "            anomalies.append(f\"Anomaly: Out of order at position {i} ({current_plot}). Expected value greater than {previous_plot}.\")\n",
    "            # Propose correction: suggest the number should be the next in sequence\n",
    "            corrected_plots[i] = previous_plot + 1\n",
    "\n",
    "        # Case 3: A number is repeated in non-sequential lines\n",
    "        if current_plot == previous_plot and i > 1 and plot_numbers[i-2] != current_plot:\n",
    "            anomalies.append(f\"Anomaly: Repeated number {current_plot} at position {i}, non-sequential.\")\n",
    "            # Propose correction: suggest the number should be the next in sequence\n",
    "            corrected_plots[i] = previous_plot\n",
    "            \n",
    "    # Propose minimal corrections for anomalies detected\n",
    "    corrections = []\n",
    "    for i, anomaly in enumerate(anomalies):\n",
    "        # Simply add the corrected plot number\n",
    "        corrections.append({\n",
    "            'anomaly': anomaly,\n",
    "            'corrected_value': corrected_plots[i]\n",
    "        })\n",
    "    \n",
    "    # Return anomalies and corrected values\n",
    "    return corrections, corrected_plots\n",
    "\n",
    "def separate_digits_and_text(input_string):\n",
    "    digits = ''\n",
    "    text = ''\n",
    "    \n",
    "    for char in input_string:\n",
    "        if char.isdigit():\n",
    "            digits += char\n",
    "        else:\n",
    "            text += char\n",
    "    return digits, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1ad1885-9be1-4bf2-9216-c3e9c8bd663b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'anomaly': 'Anomaly: Repeated number 73 at position 2, non-sequential.',\n",
       "   'corrected_value': 71},\n",
       "  {'anomaly': 'Anomaly: Repeated number 75 at position 5, non-sequential.',\n",
       "   'corrected_value': 73},\n",
       "  {'anomaly': 'Anomaly: Out of order at position 8 (59). Expected value greater than 78.',\n",
       "   'corrected_value': 73},\n",
       "  {'anomaly': 'Anomaly: Out of order at position 9 (50). Expected value greater than 59.',\n",
       "   'corrected_value': 73},\n",
       "  {'anomaly': 'Anomaly: Out of order at position 11 (52). Expected value greater than 81.',\n",
       "   'corrected_value': 75}],\n",
       " [71, 73, 73, 73, 75, 75, 76, 78, 79, 60, 81, 82])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with the provided sequence\n",
    "plots_example = [\"71\", \"73\", \"73\", \"73\", \"75\", \"75\", \"76\", \"78\", \"59\", \"50\", \"81\", \"52\"]\n",
    "anomalies, corrected_plots = detect_anomalies_and_propose_corrections(plots_example)\n",
    "anomalies, corrected_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5928ad6-13a6-4111-8b96-66aa06636bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies, corrected_plots = detect_anomalies_and_propose_corrections(corrected_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5201f57e-eebd-4607-bd1a-5a3ff3144124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71, 73, 73, 73, 75, 75, 76, 78, 79, 80, 81, 82]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1fb91e3-e3ba-445e-bcf5-0a759d8fac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/STual/DAN-cadastre/inference/LHAY/3435dedc-ddc0-4dfd-b834-d04148e44ddf.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/79ee0e05-59c0-4770-a20e-326574627577.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/683e7c3e-e537-44ba-8a02-3c4678d86b2f.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/7cc717f5-4612-45f0-8181-7e341e937880.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/9e52f7f9-0e66-4753-bcf3-d3cb0916678c.json\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(page[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m\"\u001b[39m])):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m CELL_ENTITY_TOKEN \u001b[38;5;129;01min\u001b[39;00m page[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 36\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mcorrected_plots\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m+\u001b[39m texts[i]\n\u001b[1;32m     37\u001b[0m         res \u001b[38;5;241m=\u001b[39m NormalizeText\u001b[38;5;241m.\u001b[39mreplace_characters(res, remove_chars_regex, replacement_char)\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m numbers[i] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m10000000\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "column_values = []\n",
    "CELL_ENTITY_TOKEN = \"Ⓕ\"\n",
    "remove_chars_regex = '→()↑↓×.,!?;:-@#$%^&* '\n",
    "replacement_char = ''\n",
    "\n",
    "for JSON in JSONS:\n",
    "    print(JSON)\n",
    "    with open(JSON) as f:\n",
    "        page = json.load(f)\n",
    "    page_uuid = JSON.replace(ROOT,\"\").replace('.json','')\n",
    "\n",
    "    numbers, texts = [], []\n",
    "    for line in page[\"entities\"]:\n",
    "        keys = line.keys()\n",
    "        if CELL_ENTITY_TOKEN in keys:\n",
    "            line[CELL_ENTITY_TOKEN][\"postcorrected_text\"] = ''\n",
    "            if \"interpreted_text\" in list(line[CELL_ENTITY_TOKEN].keys()):\n",
    "                num, text = separate_digits_and_text(line[CELL_ENTITY_TOKEN][\"interpreted_text\"])\n",
    "                numbers.append(num)\n",
    "                texts.append(text)\n",
    "            else:\n",
    "                line[CELL_ENTITY_TOKEN][\"interpreted_text\"] = \"\"\n",
    "        else:\n",
    "            numbers.append(0)\n",
    "            texts.append('MISSING')\n",
    "    anomalies, corrected_plots = detect_anomalies_and_propose_corrections(numbers)\n",
    "    anomalies, corrected_plots = detect_anomalies_and_propose_corrections(corrected_plots)\n",
    "\n",
    "    \n",
    "    for i in range(len(page[\"entities\"])):\n",
    "        if CELL_ENTITY_TOKEN in page[\"entities\"][i].keys():\n",
    "            res = str(corrected_plots[i]) + texts[i]\n",
    "            res = NormalizeText.replace_characters(res, remove_chars_regex, replacement_char)\n",
    "            if numbers[i] != 10000000:\n",
    "                page[\"entities\"][i][CELL_ENTITY_TOKEN][\"postcorrected_text\"] = res\n",
    "            else:\n",
    "                page[\"entities\"][i][CELL_ENTITY_TOKEN][\"postcorrected_text\"] = page[\"entities\"][i][CELL_ENTITY_TOKEN][\"interpreted_text\"]\n",
    "    \n",
    "    with open(JSON,'w', encoding='utf-8') as f:\n",
    "        json.dump(page, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee55363-a8bd-4e45-af0d-8dba2797b906",
   "metadata": {},
   "source": [
    "## 4. URIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6122f8ff-fcca-4b6d-a2fa-ef1ec0418e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/STual/DAN-cadastre/inference/LHAY/3ab13de2-f0b2-436f-9aa0-3b040e79975b.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/6b08bf95-e3f5-423c-afa2-b088cc0ca11a.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/3435dedc-ddc0-4dfd-b834-d04148e44ddf.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/79ee0e05-59c0-4770-a20e-326574627577.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/bd3d83f2-e9d7-4311-a9eb-c528f4501f27.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/683e7c3e-e537-44ba-8a02-3c4678d86b2f.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/7cc717f5-4612-45f0-8181-7e341e937880.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/9e52f7f9-0e66-4753-bcf3-d3cb0916678c.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/2c59c9c6-5ed5-4fc1-85cd-4d162c324239.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/8d60076b-985d-4374-acb8-c8f17db50d76.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/f10c1378-c307-4b24-95f8-1c016610d4af.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/92fb30f9-6891-46e0-adb0-ad396a3ad2dc.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/ba86e3f2-7bb6-49a8-a2c2-0773e0a79626.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/b9996140-18ce-4374-8a66-01c4acdcbb2f.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/d674fbbe-3b26-4e9d-8a6f-c8c8533d3123.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/0dcdaeaf-a151-4496-9d68-9e30b63270f8.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/c8299489-3750-499c-a924-7f3365401c75.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/8de045eb-0069-4e8d-a519-df5f3c51e919.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/f937f814-1950-48e0-a095-bb61ee89b2f3.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/9fde8f6c-f4bd-400c-9710-94f268917bf8.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/048cfa02-e685-4c21-94d9-93b9318be0f5.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/724852d2-464c-4364-a051-8a4505d6cbea.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/b38e1412-734c-423a-8abc-6758bff52e19.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/8a6ef849-6266-415b-b9e7-b1604f3f41d4.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/1f14549b-9b2e-4ee7-a3c4-c55b126db8af.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/2e198618-c734-4d73-a67f-79fda49504f5.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/b5e51a0b-23e8-4b63-a074-3ad00f40ecda.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/0d2dbad9-68dd-4c66-a44d-87980720bc14.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/9f05c03f-8178-4c88-8260-7934b59a1722.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/5f985c84-34b8-4e56-b125-e2d454ade11f.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/a7067b75-0b1a-42ad-a01f-5bfe6ce8161e.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/9d6d69fc-cc55-4007-acbc-4ec375bec1b3.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/d9678cb5-6ee1-4fda-9963-83108030d9b8.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/1741effa-4421-417b-8f76-95108fe49c10.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/d7b0141b-7950-4d9b-a947-92728f613d8d.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/eb6486bb-623d-4ddf-a704-7cebf29e730c.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/ad913b91-539f-4645-bcde-69dee0c09bb3.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/064cc4ee-9d5e-4922-a6dc-954aaf38fab2.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/26d52fc2-aafc-4e58-91f8-68bd6851ad37.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/3e7683fb-6de4-4121-80b2-b1bae28bc950.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/484bd9b3-8084-4bd0-bec6-1bf0ab4907e5.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/eb4fc8e4-abb6-4b61-b3cc-3b244611e66f.json\n",
      "/home/STual/DAN-cadastre/inference/LHAY/acb5c0fb-1901-405f-862e-f0726e867e7d.json\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "ADD_LINE_URIS = False\n",
    "\n",
    "if ADD_LINE_URIS:\n",
    "    for JSON in JSONS:\n",
    "        print(JSON)\n",
    "        with open(JSON) as f:\n",
    "            page = json.load(f)\n",
    "        page_uuid = JSON.replace(ROOT,\"\").replace('.json','')\n",
    "    \n",
    "        numbers, texts = [], []\n",
    "        for line in page[\"entities\"]:\n",
    "            line[\"uri\"] = str(uuid.uuid4())\n",
    "        \n",
    "        with open(JSON,'w', encoding='utf-8') as f:\n",
    "            json.dump(page, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e3a1f-eee0-4840-81c8-703bc5e6fbf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
